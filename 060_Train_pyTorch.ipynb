{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "##### Copyright (C) Microsoft Corporation.  \n",
    "see license file for details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple displays per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/azureml-share/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AZUREML_NATIVE_SHARE_DIRECTORY mapping to host dir is set by _nativeSharedDirectory_ in .compute file \n",
    "\n",
    "import os\n",
    "try:\n",
    "    amlWBSharedDir = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']    \n",
    "except:\n",
    "    amlWBSharedDir = ''\n",
    "    print('not using aml services?')\n",
    "    \n",
    "amlWBSharedDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data needs 2 things\n",
    "## TEMP (Get images)\n",
    "#crt_container  = 'https://chestxray.blob.core.windows.net/chestxraynih'\n",
    "#crt_destination = '/mnt/images'\n",
    "#answer = !yes | azcopy \\\n",
    "#    --source {crt_container} \\\n",
    "#    --destination {crt_destination} \\\n",
    "#    --recursive\n",
    "## TEMP (Get Labels csv)\n",
    "# Put to blob\n",
    "\n",
    "# Why not have a zip from blob that gets unzipped and has both images and csv?\n",
    "# Would make self-contained ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import time\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True # enables cudnn's auto-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "PyTorch:  0.3.1\n",
      "Numpy:  1.14.0\n",
      "CPUs:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "\n",
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "print(\"CPUs: \", CPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "# With small batch may be faster on P100 to do one 1 GPU\n",
    "MULTI_GPU = True\n",
    "CLASSES = 14\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "LR = 0.0001\n",
    "EPOCHS = 2 #100\n",
    "# Can scale to max for inference but for training LR will be affected\n",
    "# Prob better to increase this though on P100 since LR is not too low\n",
    "# Easier to see when plotted\n",
    "BATCHSIZE = 16 #64*2\n",
    "IMAGENET_RGB_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_RGB_SD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import utlity functions\n",
    "\n",
    "import sys, os\n",
    "paths_to_append = [os.path.join(os.getcwd(), os.path.join(*(['Code',  'src'])))]\n",
    "def add_path_to_sys_path(path_to_append):\n",
    "    if not (any(path_to_append in paths for paths in sys.path)):\n",
    "        sys.path.append(path_to_append)\n",
    "[add_path_to_sys_path(crt_path) for crt_path in paths_to_append]\n",
    "\n",
    "import azure_chestxray_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/azureml-share/chestxray/data/ChestX-ray8/ChestXray-NIHCC'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112120\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/azureml-share/chestxray/data/ChestX-ray8/ChestXray-NIHCC_other'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBox_List_2017.csv  Data_Entry_2017.csv  blacklist.csv\r\n"
     ]
    }
   ],
   "source": [
    "# create the file path variables \n",
    "# paths are tipically container level dirs mapped to a host dir for data persistence.\n",
    "\n",
    "prj_consts = azure_chestxray_utils.chestxray_consts()\n",
    "\n",
    "data_base_input_dir=os.path.join(amlWBSharedDir, \n",
    "                                 os.path.join(*(prj_consts.BASE_INPUT_DIR_list)))\n",
    "data_base_output_dir=os.path.join(amlWBSharedDir, \n",
    "                                  os.path.join(*(prj_consts.BASE_OUTPUT_DIR_list)))  \n",
    "nih_chest_xray_data_dir=os.path.join(data_base_input_dir, \n",
    "                                     os.path.join(*(prj_consts.ChestXray_IMAGES_DIR_list)))\n",
    "other_data_dir=os.path.join(data_base_input_dir, \n",
    "                            os.path.join(*(prj_consts.ChestXray_OTHER_DATA_DIR_list)))\n",
    "label_file = os.path.join(other_data_dir,'Data_Entry_2017.csv')\n",
    "\n",
    "data_partitions_dir=os.path.join(data_base_output_dir, \n",
    "                                os.path.join(*(prj_consts.DATA_PARTITIONS_DIR_list)))  \n",
    "nih_chest_xray_data_dir\n",
    "!find $nih_chest_xray_data_dir -type f | wc -l\n",
    "\n",
    "other_data_dir\n",
    "!ls $other_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "# BASE_DIR = \"/mnt\"\n",
    "# DATA_FOLDER = os.path.join(BASE_DIR, \"ChestXray-NIHCC\")\n",
    "# IMAGE_FOLDER = os.path.join(BASE_DIR, \"images\")\n",
    "# LABEL_FILE = os.path.join(DATA_FOLDER, \"Data_Entry_2017.csv\")\n",
    "# print(IMAGE_FOLDER, LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Data Loading\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # todo\n",
    "# # This should prob be a generic function\n",
    "# # Split data into train/val/test\n",
    "\n",
    "# real_total_patient_number = 30805\n",
    "# patient_id_original = [i for i in range(real_total_patient_number + 1)]\n",
    "\n",
    "# bbox_df = pd.read_csv(os.path.join(other_data_dir, 'BBox_List_2017.csv'))\n",
    "\n",
    "# black_list_set = set()\n",
    "# with open(os.path.join(other_data_dir, 'blacklist.csv'), 'r') as f:\n",
    "#     for line in f:\n",
    "#         # delete the last char which is \\n\n",
    "#         black_list_set.add(line[:-1])\n",
    "#         if int(line[:-9]) >= 30805:\n",
    "#             print(line[:-1])\n",
    "\n",
    "# # print(\"00029404_009.png\" in black_list_set)\n",
    "# bbox_patient_index_df = bbox_df['Image Index'].str.slice(3, 8)\n",
    "\n",
    "# bbox_patient_index_list = []\n",
    "\n",
    "\n",
    "# for index, item in bbox_patient_index_df.iteritems():\n",
    "#     bbox_patient_index_list.append(int(item))\n",
    "\n",
    "# patient_id = list(set(patient_id_original) - set(bbox_patient_index_list))\n",
    "# print(\"len of patient id is\", len(patient_id))\n",
    "# print(\"len of unique patient id with annotated data\", \n",
    "#       len(list(set(bbox_patient_index_list))))\n",
    "# print(\"len of patient id with annotated data\",bbox_df.shape[0])\n",
    "# print(\"len of original patient id is\", len(patient_id_original))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set fast_testing True to see the training pipeline running for a few iterations on a very small # of images\n",
    "# # set fast_testing False to perfrom real training on full training data \n",
    "# fast_testing = True\n",
    "\n",
    "# # for real training we need random order\n",
    "# if (fast_testing):\n",
    "#     shuffle_data_FLAG = False\n",
    "#     crt_patient_id = patient_id[:300]\n",
    "#     left_out_patient_id = patient_id[300:]\n",
    "# else:\n",
    "#     crt_patient_id = patient_id\n",
    "#     left_out_patient_id = []\n",
    "#     # set seed to reproduce result\n",
    "#     random.seed(0)\n",
    "#     shuffle_data_FLAG = True\n",
    "\n",
    "# # training:valid:test=7:1:2\n",
    "# train_set, other_set = train_test_split(\n",
    "#     crt_patient_id, train_size=0.7, test_size=0.3, shuffle=shuffle_data_FLAG)\n",
    "# valid_set, test_set = train_test_split(\n",
    "#     other_set, train_size=1/3, test_size=2/3, shuffle=shuffle_data_FLAG)\n",
    "# print(\"train:{} valid:{} test:{}\".format(len(train_set), len(valid_set), len(test_set)))\n",
    "\n",
    "# # test_set = test_set+left_out_patient_id\n",
    "# # print(\"train:{} valid:{} test:{}\".format(len(train_set), len(valid_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:21563 valid:3081 test:6161 nih-annotated:726\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "patient_id_partition_file = os.path.join(data_partitions_dir, 'train_test_valid_data_partitions.pickle')\n",
    "\n",
    "with open(patient_id_partition_file, 'rb') as f:\n",
    "    [train_set,valid_set,test_set, nih_annotated_set]=pickle.load(f)\n",
    "\n",
    "print(\"train:{} valid:{} test:{} nih-annotated:{}\".format(len(train_set), len(valid_set), \\\n",
    "                                                     len(test_set), len(nih_annotated_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayData(Dataset):\n",
    "    def __init__(self, img_dir, lbl_file, patient_ids, transform=None):\n",
    "        \n",
    "        # Read labels-csv\n",
    "        df = pd.read_csv(lbl_file)\n",
    "        # Filter by patient-ids\n",
    "        df = df[df['Patient ID'].isin(patient_ids)]\n",
    "        # Split labels\n",
    "        df_label = df['Finding Labels'].str.split(\n",
    "            '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "        df_label.drop(['No Finding'], axis=1, inplace=True)\n",
    "                \n",
    "        # List of images (full-path)\n",
    "        self.img_locs =  df['Image Index'].map(lambda im: os.path.join(img_dir, im)).values\n",
    "        # One-hot encoded labels (float32 for BCE loss)\n",
    "        self.labels = df_label.values\n",
    "        # Processing\n",
    "        self.transform = transform\n",
    "              \n",
    "        print(\"Loaded {} labels and {} images\".format(len(self.labels), \n",
    "                                                      len(self.img_locs)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        im_file = self.img_locs[idx]\n",
    "        im_rgb = Image.open(im_file).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform is not None:\n",
    "            im_rgb = self.transform(im_rgb)\n",
    "        return im_rgb, torch.FloatTensor(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_augmentation_dataset(img_dir, lbl_file, patient_ids, normalize):\n",
    "    dataset = XrayData(img_dir, lbl_file, patient_ids,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize(WIDTH),\n",
    "                           transforms.ToTensor(),  \n",
    "                           normalize]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 69217 labels and 69217 images\n"
     ]
    }
   ],
   "source": [
    "# Dataset for training\n",
    "# Normalise by imagenet mean/sd\n",
    "normalize = transforms.Normalize(IMAGENET_RGB_MEAN, IMAGENET_RGB_SD)\n",
    "# todo\n",
    "# Go wild here with the transforms\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/transforms/transforms.py\n",
    "#__all__ = [\"Compose\", \"ToTensor\", \"ToPILImage\", \"Normalize\", \"Resize\", \"Scale\", \"CenterCrop\", \"Pad\",\n",
    "#           \"Lambda\", \"RandomCrop\", \"RandomHorizontalFlip\", \"RandomVerticalFlip\", \"RandomResizedCrop\",\n",
    "#           \"RandomSizedCrop\", \"FiveCrop\", \"TenCrop\", \"LinearTransformation\", \"ColorJitter\", \"RandomRotation\",\n",
    "#           \"Grayscale\", \"RandomGrayscale\"]\n",
    "train_dataset = XrayData(img_dir=nih_chest_xray_data_dir,\n",
    "                         lbl_file=label_file,\n",
    "                         patient_ids=train_set,\n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.Resize(264),\n",
    "                             transforms.RandomHorizontalFlip(),\n",
    "                             transforms.RandomResizedCrop(size=WIDTH),\n",
    "                             transforms.ColorJitter(0.15, 0.15),\n",
    "                             transforms.RandomRotation(15),\n",
    "                             transforms.ToTensor(),  # need to convert image to tensor!\n",
    "                             normalize]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9600 labels and 9600 images\n",
      "Loaded 33303 labels and 33303 images\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = no_augmentation_dataset(nih_chest_xray_data_dir, label_file, valid_set, normalize)\n",
    "test_dataset = no_augmentation_dataset(nih_chest_xray_data_dir, label_file, test_set, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Helper Functions\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol(out_features=CLASSES, multi_gpu=MULTI_GPU):\n",
    "    model = models.densenet.densenet121(pretrained=True)\n",
    "    # Replace classifier (FC-1000) with (FC-14)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(model.classifier.in_features, out_features), \n",
    "        nn.Sigmoid())\n",
    "    if multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "    # CUDA\n",
    "    model.cuda()  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_symbol(sym, lr=LR):\n",
    "    # torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    opt = optim.Adam(sym.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = ReduceLROnPlateau(opt, factor = 0.1, patience = 5, mode = 'min')\n",
    "    return opt, criterion, scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_auc(data_gt, data_pd, mean=True, classes=CLASSES):\n",
    "    roc_auc = []\n",
    "    data_gt = data_gt.cpu().numpy()\n",
    "    data_pd = data_pd.cpu().numpy()\n",
    "    for i in range(classes):\n",
    "        roc_auc.append(roc_auc_score(data_gt[:, i], data_pd[:, i]))\n",
    "    if mean:\n",
    "        roc_auc = np.mean(roc_auc)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, epoch, batch=BATCHSIZE):\n",
    "    model.train()\n",
    "    print(\"Training epoch {}\".format(epoch+1))\n",
    "    loss_val = 0\n",
    "    loss_cnt = 0\n",
    "    for data, target in dataloader:\n",
    "        # Get samples\n",
    "        data = Variable(torch.FloatTensor(data).cuda())\n",
    "        target = Variable(torch.FloatTensor(target).cuda())\n",
    "        # Init\n",
    "        optimizer.zero_grad()\n",
    "        # Forwards\n",
    "        output = model(data)\n",
    "        # Loss\n",
    "        loss = criterion(output, target)\n",
    "        # Back-prop\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "         # Log the loss\n",
    "        loss_val += loss.data[0]\n",
    "        loss_cnt += 1\n",
    "    print(\"Training loss: {0:.4f}\".format(loss_val/loss_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, dataloader, criterion, epoch, phase='valid', batch=BATCHSIZE):\n",
    "    model.eval()\n",
    "    if phase == 'testing':\n",
    "        print(\"Testing epoch {}\".format(epoch+1))\n",
    "    else:\n",
    "        print(\"Validating epoch {}\".format(epoch+1))\n",
    "    out_pred = torch.FloatTensor().cuda()\n",
    "    out_gt = torch.FloatTensor().cuda()\n",
    "    loss_val = 0\n",
    "    loss_cnt = 0\n",
    "    for data, target in dataloader:\n",
    "        # Get samples\n",
    "        data = Variable(torch.FloatTensor(data).cuda(), volatile=True)\n",
    "        target = Variable(torch.FloatTensor(target).cuda(), volatile=True)\n",
    "         # Forwards\n",
    "        output = model(data)\n",
    "        # Loss\n",
    "        loss = criterion(output, target)\n",
    "        # Log the loss\n",
    "        loss_val += loss.data[0]\n",
    "        loss_cnt += 1\n",
    "        # Log for AUC\n",
    "        out_pred = torch.cat((out_pred, output.data), 0)\n",
    "        out_gt = torch.cat((out_gt, target.data), 0)\n",
    "    loss_mean = loss_val/loss_cnt\n",
    "    if phase == 'testing':\n",
    "        print(\"Test-Dataset loss: {0:.4f}\".format(loss_mean))\n",
    "        print(\"Test-Dataset AUC: {0:.4f}\".format(compute_roc_auc(out_gt, out_pred)))\n",
    "\n",
    "    else:\n",
    "        print(\"Validation loss: {0:.4f}\".format(loss_mean))\n",
    "        print(\"Validation AUC: {0:.4f}\".format(compute_roc_auc(out_gt, out_pred)))\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_learning_rate(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        print(\"Learining rate: \", param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "# num_workers=4*CPU_COUNT\n",
    "# pin_memory=True\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCHSIZE,\n",
    "                          shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=8*BATCHSIZE,\n",
    "                          shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=8*BATCHSIZE,\n",
    "                         shuffle=False, num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Train Azure Chest Xray\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /home/mmlspark/.torch/models/densenet121-a639ec97.pth\n",
      "100%|██████████| 32342954/32342954 [00:00<00:00, 55784863.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 s, sys: 1.01 s, total: 3.66 s\n",
      "Wall time: 3.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load symbol\n",
    "azure_chest_xray_sym = get_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.89 ms, sys: 240 µs, total: 2.13 ms\n",
      "Wall time: 2.14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load optimiser, loss\n",
    "optimizer, criterion, scheduler = init_symbol(azure_chest_xray_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 21 13:05:22 2018       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 0000C3D4:00:00.0 Off |                  Off |\n",
      "| N/A   36C    P0    72W / 149W |    241MiB / 12205MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           Off  | 0000DCAE:00:00.0 Off |                  Off |\n",
      "| N/A   41C    P8    26W / 149W |     11MiB / 12205MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "CUDA Version 8.0.61\n",
      "CUDA Version 8.0.61\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!cat /usr/local/cuda-8.0/version.txt\n",
    "!cat /usr/local/cuda/version.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating epoch 0\n",
      "Validation loss: 0.5749\n",
      "Validation AUC: 0.5071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5748794953028361"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "Training loss: 0.1596\n",
      "Validating epoch 1\n",
      "Validation loss: 0.1471\n",
      "Validation AUC: 0.7662\n",
      "Testing epoch 1\n",
      "Test-Dataset loss: 0.1952\n",
      "Test-Dataset AUC: 0.7568\n",
      "Learining rate:  0.0001\n",
      "Loss decreased. Saving ...\n",
      "Epoch time: 9170 seconds\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Training epoch 2\n",
      "Training loss: 0.1516\n",
      "Validating epoch 2\n",
      "Validation loss: 0.1464\n",
      "Validation AUC: 0.7875\n",
      "Testing epoch 2\n",
      "Test-Dataset loss: 0.1908\n",
      "Test-Dataset AUC: 0.7820\n",
      "Learining rate:  0.0001\n",
      "Loss decreased. Saving ...\n",
      "Epoch time: 13616 seconds\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# Original CheXNet ROC AUC = 0.841\n",
    "loss_min = float(\"inf\")    \n",
    "stime = time.time()\n",
    "\n",
    "# No-training\n",
    "valid_epoch(azure_chest_xray_sym, valid_loader, criterion, -1)\n",
    "\n",
    "# Main train/val/test loop\n",
    "for j in range(EPOCHS):\n",
    "    train_epoch(azure_chest_xray_sym, train_loader, optimizer, criterion, j)\n",
    "    loss_val = valid_epoch(azure_chest_xray_sym, valid_loader, criterion, j)\n",
    "    test_loss_val = valid_epoch(azure_chest_xray_sym, test_loader, criterion, j, 'testing')\n",
    "    # LR Schedule\n",
    "    scheduler.step(loss_val)\n",
    "    print_learning_rate(optimizer)\n",
    "    # todo: tensorboard hooks\n",
    "    # Logging\n",
    "    if loss_val < loss_min:\n",
    "        print(\"Loss decreased. Saving ...\")\n",
    "        loss_min = loss_val\n",
    "        torch.save({'epoch': j + 1, \n",
    "                    'state_dict': azure_chest_xray_sym.state_dict(), \n",
    "                    'best_loss': loss_min, \n",
    "                    'optimizer' : optimizer.state_dict()}, 'best_azure_chest_xray_model_v2.pth.tar')\n",
    "    etime = time.time()\n",
    "    print(\"Epoch time: {0:.0f} seconds\".format(etime-stime))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Test azure_chest_xray\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for testing\n",
    "azure_chest_xray_sym_test = get_symbol()\n",
    "chkpt = torch.load(\"best_azure_chest_xray_model_v2.pth.tar\")\n",
    "azure_chest_xray_sym_test.load_state_dict(chkpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating epoch 0\n",
      "Validation loss: 0.1464\n",
      "Validation AUC: 0.7875\n",
      "Testing epoch 0\n",
      "Test-Dataset loss: 0.1908\n",
      "Test-Dataset AUC: 0.7820\n"
     ]
    }
   ],
   "source": [
    "valid_loss = valid_epoch(azure_chest_xray_sym_test, valid_loader, criterion, -1)\n",
    "test_loss = valid_epoch(azure_chest_xray_sym_test, test_loader, criterion, -1, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.onnx\n",
    "#dummy_input = Variable(torch.randn(BATCHSIZE, CHANNELS, HEIGHT, WIDTH)).cuda()\n",
    "#torch.onnx.export(azure_chest_xray_sym_test, dummy_input, \"azure_chest_xray.proto\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to html .\\Code\\02_Model\\060_Train_pyTorch.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
